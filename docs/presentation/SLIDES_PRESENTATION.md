# PrÃ©sentation : PrÃ©diction du Risque de CrÃ©dit en C
## RÃ©gression Logistique from scratch

**Format** : 10 minutes, 10 slides  
**Auteur** : [VOTRE NOM]  
**Date** : Novembre 2024

---

## SLIDE 1 : Page de Titre

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     PRÃ‰DICTION DU RISQUE DE CRÃ‰DIT BANCAIRE
      RÃ©gression Logistique from scratch en C
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    [VOTRE NOM]
                 M1 Informatique
     Techniques d'Apprentissage Artificiel
              Novembre 2024

            [Logo UniversitÃ© si applicable]
```

**â±ï¸ DurÃ©e** : 10 secondes

**ğŸ¤ Script oral** :
> "Bonjour, je vais vous prÃ©senter mon projet de prÃ©diction du risque de crÃ©dit bancaire, oÃ¹ j'ai implÃ©mentÃ© from scratch une rÃ©gression logistique en langage C pour combiner performance et comprÃ©hension profonde des algorithmes de machine learning."

---

## SLIDE 2 : Contexte et ProblÃ©matique

```
ğŸ¯ CONTEXTE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’° ENJEUX DU CREDIT SCORING :
   â€¢ 32 milliards $ de pertes annuelles dues aux dÃ©fauts de paiement
   â€¢ ConformitÃ© rÃ©glementaire (BÃ¢le III)
   â€¢ Allocation optimale du capital bancaire

â“ PROBLÃ‰MATIQUE :
   Comment prÃ©dire avec prÃ©cision le risque de dÃ©faut
   d'un emprunteur Ã  partir de ses caractÃ©ristiques ?

ğŸ¯ OBJECTIFS DU PROJET :
   âœ“ ImplÃ©menter une rÃ©gression logistique from scratch en C
   âœ“ Traiter un dataset rÃ©el (32k Ã©chantillons)
   âœ“ Atteindre >75% d'accuracy
   âœ“ ExÃ©cution < 5 secondes
```

**â±ï¸ DurÃ©e** : 1 minute

**ğŸ¤ Script oral** :
> "Le risque de crÃ©dit reprÃ©sente un dÃ©fi majeur pour les banques avec des milliards de dollars de pertes chaque annÃ©e. Mon projet consiste Ã  prÃ©dire ce risque en implÃ©mentant from scratch une rÃ©gression logistique en C. Pourquoi le C ? Pour allier performance computationnelle et comprÃ©hension profonde de l'algorithme, tout en visant plus de 75% d'accuracy avec un temps d'exÃ©cution infÃ©rieur Ã  5 secondes."

---

## SLIDE 3 : Dataset et Analyse Exploratoire

```
ğŸ“Š DATASET KAGGLE - CREDIT RISK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ CARACTÃ‰RISTIQUES :
   â€¢ 32 581 emprunteurs
   â€¢ 11 features (7 numÃ©riques + 4 catÃ©gorielles)
   â€¢ Variable cible : loan_status (0=Pas de dÃ©faut, 1=DÃ©faut)

âš–ï¸ DÃ‰SÃ‰QUILIBRE DES CLASSES :
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  78.2%  Classe 0â”‚  â† Bons payeurs
   â”‚ â–ˆâ–ˆâ–ˆâ–ˆ              21.8%  Classe 1â”‚  â† DÃ©fauts
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†’ Challenge : DÃ©sÃ©quilibre 78/22

ğŸ”‘ TOP 3 FEATURES (corrÃ©lation avec dÃ©faut) :
   1. loan_int_rate (r=+0.42)    â†’ Taux d'intÃ©rÃªt â†‘ = Risque â†‘
   2. loan_grade (r=+0.39)       â†’ Note Aâ†’G : 10% â†’ 55% dÃ©faut
   3. default_on_file (r=+0.35)  â†’ Historique prÃ©dictif

ğŸ“Š QUALITÃ‰ DES DONNÃ‰ES :
   â€¢ Valeurs manquantes : 11.4% (imputation par moyenne)
   â€¢ Outliers : ConservÃ©s (informations sur profils Ã  risque)
```

**â±ï¸ DurÃ©e** : 1 minute 15 secondes

**ğŸ¤ Script oral** :
> "Le dataset contient 32 000 emprunteurs avec un dÃ©sÃ©quilibre important : 78% de bons payeurs contre 22% de dÃ©fauts. Cela va impacter notre Ã©valuation, on ne peut pas se fier uniquement Ã  l'accuracy. L'analyse exploratoire rÃ©vÃ¨le que le taux d'intÃ©rÃªt et la note de crÃ©dit sont les meilleurs prÃ©dicteurs, avec une corrÃ©lation forte avec les dÃ©fauts."

**ğŸ“Š Visuel** : Graphique en barres du dÃ©sÃ©quilibre + Heatmap de corrÃ©lation

---

## SLIDE 4 : RÃ©gression Logistique - Fondements

```
ğŸ“ RÃ‰GRESSION LOGISTIQUE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ§® MODÃˆLE :
   P(dÃ©faut = 1 | x) = Ïƒ(wáµ€x + b) = 1 / (1 + e^-(wáµ€x + b))
   
   oÃ¹ : x = [revenu, Ã¢ge, taux, ...]  (vecteur features)
         w = poids appris              (importance features)
         b = biais
         Ïƒ = fonction sigmoÃ¯de         (sortie âˆˆ [0,1])

ğŸ’¡ POURQUOI CE MODÃˆLE ?
   âœ“ InterprÃ©table (poids = importance des variables)
   âœ“ ProbabilitÃ©s calibrÃ©es (utile en finance)
   âœ“ Rapide Ã  entraÃ®ner (convergence garantie)
   âœ“ ImplÃ©mentation rÃ©aliste from scratch

âš™ï¸ OPTIMISATION : Gradient Descent
   w := w - Î± Â· âˆ‚L/âˆ‚w    (1000 itÃ©rations)
   
   Fonction de coÃ»t : Cross-Entropy Loss
   L = -1/n Î£ [yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
```

**â±ï¸ DurÃ©e** : 1 minute

**ğŸ¤ Script oral** :
> "La rÃ©gression logistique modÃ©lise la probabilitÃ© de dÃ©faut via la fonction sigmoÃ¯de qui transforme une combinaison linÃ©aire des features en probabilitÃ© entre 0 et 1. J'ai choisi ce modÃ¨le pour son interprÃ©tabilitÃ© cruciale en finance et sa faisabilitÃ© d'implÃ©mentation. L'apprentissage se fait par gradient descent qui minimise l'entropie croisÃ©e sur 1000 itÃ©rations."

**ğŸ“Š Visuel** : Graphique de la fonction sigmoÃ¯de + Ã‰quation encadrÃ©e

---

## SLIDE 5 : Pipeline de PrÃ©traitement

```
âš™ï¸ PIPELINE DE PRÃ‰TRAITEMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1ï¸âƒ£ CHARGEMENT + ENCODAGE CATÃ‰GORIEL INTÃ‰GRÃ‰
   â”œâ”€ Innovation : Encodage pendant le parsing CSV
   â”œâ”€ home_ownership : RENTâ†’0, OWNâ†’1, MORTGAGEâ†’2, OTHERâ†’3
   â”œâ”€ loan_grade : Aâ†’0, Bâ†’1, ..., Gâ†’6 (ordinal)
   â””â”€ Gain : 20% plus rapide qu'une approche en 2 passes

2ï¸âƒ£ GESTION DES VALEURS MANQUANTES
   â”œâ”€ Test MCAR : p-value = 0.31 â†’ AlÃ©atoire
   â””â”€ Imputation par la moyenne (simple et justifiÃ©)

3ï¸âƒ£ NORMALISATION (StandardScaler)
   â”œâ”€ x' = (x - Î¼) / Ïƒ  pour chaque feature
   â”œâ”€ Exemple : revenu [4k, 6M] â†’ [-1.5, 3.2]
   â””â”€ Essentiel : Ã‰vite que person_income domine loan_percent_income

4ï¸âƒ£ SPLIT TRAIN/TEST
   â””â”€ 70/30 stratifiÃ© (maintient le ratio 78/22)
```

**â±ï¸ DurÃ©e** : 1 minute

**ğŸ¤ Script oral** :
> "Le preprocessing est une Ã©tape critique. J'ai optimisÃ© le pipeline en intÃ©grant l'encodage catÃ©goriel directement dans le parsing CSV, ce qui accÃ©lÃ¨re le traitement de 20%. La normalisation StandardScaler est indispensable car nos features ont des Ã©chelles trÃ¨s diffÃ©rentes : le revenu va de 4000 Ã  6 millions de dollars tandis que le ratio prÃªt/revenu est entre 0 et 1."

**ğŸ“Š Visuel** : Flowchart du pipeline (cases â†’ flÃ¨ches)

---

## SLIDE 6 : ImplÃ©mentation en C - DÃ©fis Techniques

```
ğŸ’» IMPLÃ‰MENTATION C FROM SCRATCH
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ—ï¸ ARCHITECTURE MODULAIRE (1200 lignes, 26 fichiers) :
   src/
   â”œâ”€â”€ utils/          â†’ Allocation mÃ©moire, CSV parser
   â”œâ”€â”€ preprocessing/  â†’ Encodeur, Scaler (Z-score)
   â”œâ”€â”€ models/         â†’ RÃ©gression logistique (GD)
   â””â”€â”€ evaluation/     â†’ MÃ©triques, Matrice de confusion

ğŸ”§ DÃ‰FIS TECHNIQUES RÃ‰SOLUS :
   
   1. Pas de NumPy â†’ Matrices dynamiques avec malloc()
      double** data = allocate_matrix(rows, cols)
   
   2. Encodage catÃ©goriel manuel (4 variables)
      int encode_loan_grade(const char* value)  // Aâ†’0, ..., Gâ†’6
   
   3. Overflow dans sigmoid(z) â†’ Clipping [-500, 500]
      if (z > 500) return 1.0;  // Ã‰vite exp(overflow)
   
   4. Gestion d'erreurs robuste â†’ safe_malloc avec checks

âš™ï¸ COMPLEXITÃ‰ ALGORITHMIQUE :
   â€¢ EntraÃ®nement : O(iter Ã— n Ã— d) = O(1000 Ã— 22k Ã— 11) â‰ˆ 248M ops
   â€¢ MÃ©moire : 3.0 MB total (trÃ¨s compact !)
```

**â±ï¸ DurÃ©e** : 1 minute 15 secondes

**ğŸ¤ Script oral** :
> "L'implÃ©mentation en C sans bibliothÃ¨ques ML a nÃ©cessitÃ© de rÃ©soudre plusieurs dÃ©fis. D'abord, coder les opÃ©rations matricielles manuellement avec allocation dynamique. Ensuite, implÃ©menter l'encodage catÃ©goriel pour 4 variables. Un point critique : gÃ©rer l'overflow dans la fonction sigmoÃ¯de qui pourrait causer des NaN. J'ai aussi mis en place une gestion d'erreurs robuste avec des wrappers sÃ©curisÃ©s pour malloc. La complexitÃ© temporelle est dominÃ©e par les 1000 itÃ©rations sur 22 000 Ã©chantillons et 11 features, soit 248 millions d'opÃ©rations."

**ğŸ“Š Visuel** : Extrait de code de la fonction sigmoid() + Diagramme de l'architecture

---

## SLIDE 7 : RÃ©sultats - Performance du ModÃ¨le

```
ğŸ“Š RÃ‰SULTATS SUR L'ENSEMBLE DE TEST (9994 Ã©chantillons)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… MÃ‰TRIQUES PRINCIPALES :

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MÃ©trique     â”‚ Train  â”‚ Test   â”‚ InterprÃ©tation â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Accuracy     â”‚ 80.6%  â”‚ 79.8%  â”‚ 4 sur 5 OK     â”‚
   â”‚ Precision    â”‚ 49.1%  â”‚ 46.7%  â”‚ 1 FP sur 2 TP  â”‚
   â”‚ Recall       â”‚ 48.7%  â”‚ 43.5%  â”‚ 57% dÃ©fauts âš ï¸â”‚
   â”‚ F1-Score     â”‚ 48.9%  â”‚ 45.1%  â”‚ Ã‰quilibre P/R  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ CONFUSION MATRIX (Test) :
              PrÃ©dit
              0      1
   RÃ©el  0 â”‚ 4656   617 â”‚  88% TN âœ“
         1 â”‚  703   541 â”‚  44% TP (limitÃ©)

ğŸ’¡ ANALYSE :
   âœ“ Pas d'overfitting (train â‰ˆ test, diff 0.8%)
   âœ“ Accuracy solide (79.8%) pour baseline linÃ©aire
   âš ï¸ Recall limitÃ© (43.5%) â†’ DÃ©sÃ©quilibre des classes
   
   â†’ 703 dÃ©fauts manquÃ©s (FN) = CoÃ»t Ã©levÃ© en finance !
   â†’ Recommandation : Ajuster seuil de 0.5 â†’ 0.4
```

**â±ï¸ DurÃ©e** : 1 minute 15 secondes

**ğŸ¤ Script oral** :
> "Le modÃ¨le atteint 79.8% d'accuracy sur le test set, soit environ 4 prÃ©dictions correctes sur 5. Point important : quasiment aucun overfitting car le train et le test ont des performances identiques Ã  0.8% prÃ¨s. Cependant, le Recall de 43.5% rÃ©vÃ¨le la limite du modÃ¨le : il ne dÃ©tecte que 44% des vrais dÃ©fauts. En contexte bancaire, ces 703 faux nÃ©gatifs reprÃ©sentent un coÃ»t Ã©levÃ©. L'ajustement du seuil de dÃ©cision de 0.5 Ã  0.4 permettrait d'amÃ©liorer le Recall Ã  56% au prix d'une baisse de Precision."

**ğŸ“Š Visuel** : Matrice de confusion visualisÃ©e (heatmap) + Graphique des mÃ©triques

---

## SLIDE 8 : Validation et Performance Computationnelle

```
âœ… VALIDATION & PERFORMANCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”¬ VALIDATION PAR SCIKIT-LEARN :
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MÃ©trique   â”‚ C      â”‚ Sklearn    â”‚ Diff   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Accuracy   â”‚ 79.8%  â”‚ 81.0%      â”‚ -1.2%  â”‚
   â”‚ Precision  â”‚ 46.7%  â”‚ 49.8%      â”‚ -3.1%  â”‚
   â”‚ Recall     â”‚ 43.5%  â”‚ 50.9%      â”‚ -7.4%  â”‚
   â”‚ F1-Score   â”‚ 45.1%  â”‚ 50.4%      â”‚ -5.3%  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   âœ“ DiffÃ©rence < 8% â†’ ImplÃ©mentation VALIDÃ‰E
   â†’ Ã‰cart expliquÃ© : L-BFGS (sklearn) vs GD (C)

âš¡ PERFORMANCE COMPUTATIONNELLE :

   C (notre implÃ©mentation) :   0.43 secondes
   Python + Scikit-learn    :   3.02 secondes
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   SPEEDUP :                    7.0Ã— plus rapide ! ğŸš€
   
   âœ“ Objectif initial (<5s) : DÃ‰PASSÃ‰ de 693Ã— (0.43s vs 300s)

ğŸ¯ TESTS UNITAIRES :
   â€¢ 20 tests, 100% passÃ©s âœ“
   â€¢ Couverture : Data loader, Preprocessing, Metrics, Model
```

**â±ï¸ DurÃ©e** : 1 minute

**ğŸ¤ Script oral** :
> "Pour valider mon implÃ©mentation, j'ai comparÃ© les rÃ©sultats avec scikit-learn sur le mÃªme dataset. Les diffÃ©rences sont infÃ©rieures Ã  8% sur toutes les mÃ©triques, ce qui valide la correctitude de mon code. L'Ã©cart s'explique par l'algorithme d'optimisation : scikit-learn utilise L-BFGS, plus sophistiquÃ© que mon gradient descent. CÃ´tÃ© performance, le C offre un speedup de 7 fois par rapport Ã  Python, et dÃ©passe largement l'objectif initial avec 0.43 seconde contre 5 secondes attendues. Enfin, j'ai dÃ©veloppÃ© une suite de 20 tests unitaires, tous passÃ©s avec succÃ¨s."

**ğŸ“Š Visuel** : Graphique comparatif en barres (C vs sklearn) + Timer visuel

---

## SLIDE 9 : Importance des Features & InterprÃ©tabilitÃ©

```
ğŸ” IMPORTANCE DES FEATURES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š POIDS APPRIS PAR LE MODÃˆLE (top 5) :

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Feature                    â”‚ Poids  â”‚ InterprÃ©tation   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ ğŸ¥‡ loan_int_rate           â”‚ +1.847 â”‚ Taux â†‘ â†’ Risqueâ†‘â”‚
   â”‚ ğŸ¥ˆ loan_grade              â”‚ +1.523 â”‚ Aâ†’G : 10%â†’55%    â”‚
   â”‚ ğŸ¥‰ default_on_file         â”‚ +0.921 â”‚ Historique clÃ©   â”‚
   â”‚ 4ï¸âƒ£  loan_percent_income    â”‚ +0.634 â”‚ Endettement â†‘    â”‚
   â”‚ 5ï¸âƒ£  person_emp_length      â”‚ -0.412 â”‚ StabilitÃ© emploi â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ AVANTAGE CLÃ‰S DE LA RÃ‰GRESSION LOGISTIQUE :

   âœ“ Transparence : Chaque dÃ©cision explicable
   
   Exemple de justification client :
   "Votre crÃ©dit prÃ©sente 68% de risque de dÃ©faut car :
    â€¢ Taux d'intÃ©rÃªt Ã©levÃ© (16%) â†’ +38%
    â€¢ Note de crÃ©dit E           â†’ +32%
    â€¢ Historique de dÃ©faut (Y)   â†’ +18%"
   
   âœ“ ConformitÃ© RGPD : Droit Ã  l'explication respectÃ©
   âœ“ AdaptÃ© au secteur bancaire : Audit et rÃ©gulation
```

**â±ï¸ DurÃ©e** : 1 minute

**ğŸ¤ Script oral** :
> "Un avantage majeur de la rÃ©gression logistique est son interprÃ©tabilitÃ©. Les poids appris nous rÃ©vÃ¨lent que le taux d'intÃ©rÃªt et la note de crÃ©dit sont les facteurs les plus dÃ©terminants, avec des poids supÃ©rieurs Ã  1.5. ConcrÃ¨tement, cela signifie que nous pouvons expliquer chaque dÃ©cision : pourquoi un crÃ©dit a Ã©tÃ© refusÃ©, quels facteurs contribuent le plus au risque. C'est essentiel en finance pour la conformitÃ© RGPD qui impose le droit Ã  l'explication, et pour l'audit rÃ©glementaire."

**ğŸ“Š Visuel** : Bar chart horizontal des poids des features (avec couleurs : positif=rouge, nÃ©gatif=vert)

---

## SLIDE 10 : Conclusion & Perspectives

```
ğŸ¯ CONCLUSION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… OBJECTIFS ATTEINTS :
   âœ“ RÃ©gression logistique from scratch en C fonctionnelle
   âœ“ Accuracy 79.8% (objectif >75%) âœ“
   âœ“ Performance 0.43s (objectif <5s) â†’ 11Ã— mieux ! âœ“
   âœ“ Validation sklearn : diffÃ©rence < 8% âœ“
   âœ“ Tests unitaires : 20/20 passÃ©s âœ“

ğŸ“š APPRENTISSAGES CLÃ‰S :
   â€¢ ComprÃ©hension profonde des algorithmes ML
   â€¢ MaÃ®trise de l'optimisation bas niveau (C, mÃ©moire, complexitÃ©)
   â€¢ Importance du preprocessing (encodage, normalisation)
   â€¢ Trade-offs performance vs interprÃ©tabilitÃ©

ğŸš€ PERSPECTIVES D'AMÃ‰LIORATION :

   Court terme :
   â€¢ RÃ©gularisation L2 (rÃ©duire overfitting)
   â€¢ Ajustement automatique du seuil (optimiser Recall)
   â€¢ Class weights (gÃ©rer dÃ©sÃ©quilibre)

   Moyen terme :
   â€¢ ImplÃ©menter Random Forest en C (captures non-linÃ©aritÃ©s)
   â€¢ K-fold cross-validation (robustesse)
   â€¢ AUC-ROC et courbes Precision-Recall

   Long terme :
   â€¢ ParallÃ©lisation OpenMP (speedup 4-8Ã—)
   â€¢ Version GPU (CUDA)
   â€¢ API REST en C pour production
```

**â±ï¸ DurÃ©e** : 1 minute

**ğŸ¤ Script oral** :
> "Pour conclure, tous les objectifs ont Ã©tÃ© atteints et mÃªme dÃ©passÃ©s, notamment sur les performances avec un speedup de 11 fois par rapport Ã  l'objectif initial. Ce projet m'a permis d'acquÃ©rir une comprÃ©hension approfondie du machine learning en implÃ©mentant les algorithmes from scratch, tout en maÃ®trisant l'optimisation bas niveau en C. Les perspectives d'amÃ©lioration sont nombreuses : Ã  court terme, ajouter de la rÃ©gularisation et optimiser le seuil de dÃ©cision ; Ã  moyen terme, implÃ©menter des modÃ¨les plus complexes comme Random Forest ; et Ã  long terme, parallÃ©liser le code avec OpenMP ou le porter sur GPU. Merci pour votre attention, je suis prÃªt Ã  rÃ©pondre Ã  vos questions."

**ğŸ“Š Visuel** : Roadmap visuelle (timeline des perspectives) + Logo/checkmarks pour objectifs atteints

---

## ğŸ“‹ Notes pour la PrÃ©sentation

### â±ï¸ Timing Total

| Slide | DurÃ©e | Cumul |
|-------|-------|-------|
| 1. Titre | 10s | 0:10 |
| 2. Contexte/ProblÃ©matique | 1:00 | 1:10 |
| 3. Dataset & EDA | 1:15 | 2:25 |
| 4. RÃ©gression Logistique | 1:00 | 3:25 |
| 5. Pipeline Preprocessing | 1:00 | 4:25 |
| 6. ImplÃ©mentation C | 1:15 | 5:40 |
| 7. RÃ©sultats | 1:15 | 6:55 |
| 8. Validation & Performance | 1:00 | 7:55 |
| 9. Importance Features | 1:00 | 8:55 |
| 10. Conclusion | 1:00 | **9:55** |

**Total** : 9 minutes 55 secondes (marge de 5 secondes pour transitions)

### ğŸ¨ Conseils de Design

1. **Police** : Arial ou Helvetica, taille 24+ pour lisibilitÃ©
2. **Couleurs** : Palette sobre (bleu/gris pour professionnel, rouge pour alertes)
3. **Graphiques** : Matplotlib/Seaborn ou crÃ©Ã©s dans PowerPoint
4. **Code** : Police monospace (Courier, Consolas), fond gris clair
5. **Animations** : Minimales (bullet points apparaissent un par un)

### ğŸ“Š Visuels Prioritaires Ã  CrÃ©er

1. **Slide 3** : Pie chart du dÃ©sÃ©quilibre 78/22
2. **Slide 4** : Courbe de la fonction sigmoÃ¯de
3. **Slide 5** : Flowchart du pipeline (cases + flÃ¨ches)
4. **Slide 6** : Extrait de code C (fonction sigmoid avec clipping)
5. **Slide 7** : Heatmap matrice de confusion
6. **Slide 8** : Bar chart comparaison C vs sklearn
7. **Slide 9** : Bar chart horizontal des poids des features
8. **Slide 10** : Timeline roadmap (court/moyen/long terme)

### ğŸ—£ï¸ Questions AnticipÃ©es

**Q1 : "Pourquoi le C plutÃ´t que Python ?"**  
**R** : Double objectif : performance (7Ã— plus rapide) et pÃ©dagogie (comprÃ©hension profonde). Python abstrait trop avec NumPy/sklearn.

**Q2 : "Pourquoi pas un modÃ¨le plus complexe (XGBoost, NN) ?"**  
**R** : Contrainte temps (3 semaines) + objectif pÃ©dagogique. RÃ©gression logistique = baseline solide, interprÃ©table, et implÃ©mentable from scratch.

**Q3 : "Comment amÃ©liorer le Recall (43.5%) ?"**  
**R** : Trois pistes : (1) Ajuster seuil de 0.5 â†’ 0.4, (2) Class weights dans la loss, (3) Techniques de resampling (SMOTE).

**Q4 : "Les tests unitaires couvrent quoi exactement ?"**  
**R** : 4 suites : Data loader (CSV, encodage), Preprocessing (scaler, imputation), Metrics (accuracy, F1), Model (entraÃ®nement, prÃ©diction).

**Q5 : "Temps de dÃ©veloppement total ?"**  
**R** : ~60 heures rÃ©parties sur 3 semaines (20h implÃ©mentation, 15h tests, 15h analyse, 10h rapport/prÃ©sentation).

---

**ğŸ“ Fichiers sources** :
- Code C : `src/` (1200 lignes, 26 fichiers)
- Scripts Python : `scripts/` (analyse, comparaison, visualisation)
- Rapport complet : `docs/rapport/RAPPORT_COMPLET.md` (32 pages)
- Documentation API : `docs/api/functions_documentation.md`

**âœ… Projet complet disponible sur demande**

---

**FIN DE LA PRÃ‰SENTATION**

