#!/usr/bin/env python3
"""
Comparaison de l'impl√©mentation C avec scikit-learn
Valide que les r√©sultats sont similaires
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from pathlib import Path

def load_and_preprocess_data():
    """Charge et pr√©traite les donn√©es comme dans l'impl√©mentation C"""
    print("üìÇ Chargement du dataset...")
    df = pd.read_csv("data/raw/credit_risk_dataset.csv")
    
    print(f"‚úì Dataset charg√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
    
    # Encodage des variables cat√©gorielles (comme dans encoder.c)
    print("\nüîÑ Encodage des variables cat√©gorielles...")
    
    # person_home_ownership: RENT=0, OWN=1, MORTGAGE=2, OTHER=3
    home_mapping = {'RENT': 0, 'OWN': 1, 'MORTGAGE': 2, 'OTHER': 3}
    df['person_home_ownership'] = df['person_home_ownership'].map(home_mapping)
    
    # loan_intent: PERSONAL=0, EDUCATION=1, MEDICAL=2, VENTURE=3, HOMEIMPROVEMENT=4, DEBTCONSOLIDATION=5
    intent_mapping = {'PERSONAL': 0, 'EDUCATION': 1, 'MEDICAL': 2, 
                     'VENTURE': 3, 'HOMEIMPROVEMENT': 4, 'DEBTCONSOLIDATION': 5}
    df['loan_intent'] = df['loan_intent'].map(intent_mapping)
    
    # loan_grade: A=1, B=2, C=3, D=4, E=5, F=6, G=7
    grade_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}
    df['loan_grade'] = df['loan_grade'].map(grade_mapping)
    
    # cb_person_default_on_file: N=0, Y=1
    default_mapping = {'N': 0, 'Y': 1}
    df['cb_person_default_on_file'] = df['cb_person_default_on_file'].map(default_mapping)
    
    print("‚úì Variables cat√©gorielles encod√©es")
    
    # Gestion des valeurs manquantes (remplacement par la moyenne)
    print("\nüîß Gestion des valeurs manquantes...")
    df = df.fillna(df.mean())
    print("‚úì Valeurs manquantes trait√©es")
    
    # S√©parer X et y
    X = df.drop('loan_status', axis=1)
    y = df['loan_status']
    
    return X, y

def train_sklearn_model(X_train, y_train, X_test, y_test):
    """Entra√Æne un mod√®le sklearn pour comparaison"""
    print("\nü§ñ Entra√Ænement du mod√®le scikit-learn...")
    
    # Normalisation (StandardScaler comme dans scaler.c)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # R√©gression logistique
    # Param√®tres similaires √† l'impl√©mentation C:
    # - solver='lbfgs' (optimizer)
    # - max_iter=1000 (comme dans main.c)
    model = LogisticRegression(
        solver='lbfgs',
        max_iter=1000,
        random_state=42
    )
    
    model.fit(X_train_scaled, y_train)
    
    print("‚úì Mod√®le entra√Æn√©")
    
    return model, scaler, X_train_scaled, X_test_scaled

def load_c_results():
    """Charge les r√©sultats de l'impl√©mentation C"""
    print("\nüìä Chargement des r√©sultats de l'impl√©mentation C...")
    
    results = {}
    
    # Charger les m√©triques de test
    try:
        with open("results/metrics/test_metrics.txt", 'r') as f:
            lines = f.readlines()
            for line in lines:
                if "Accuracy" in line:
                    results['c_accuracy'] = float(line.split(':')[1].strip())
                elif "Precision" in line:
                    results['c_precision'] = float(line.split(':')[1].strip())
                elif "Recall" in line:
                    results['c_recall'] = float(line.split(':')[1].strip())
                elif "F1-Score" in line:
                    results['c_f1'] = float(line.split(':')[1].strip())
        
        print("‚úì M√©triques C charg√©es")
    except FileNotFoundError:
        print("‚ö† Fichiers de r√©sultats C non trouv√©s. Ex√©cutez d'abord le programme C.")
        return None
    
    return results

def compare_results(sklearn_metrics, c_results):
    """Compare les r√©sultats sklearn vs C"""
    print("\n" + "=" * 60)
    print("COMPARAISON DES R√âSULTATS")
    print("=" * 60)
    
    if c_results is None:
        print("\n‚ö† Impossible de comparer : r√©sultats C manquants")
        return
    
    print("\n{:<20} {:<15} {:<15} {:<15}".format("M√©trique", "Scikit-learn", "C (custom)", "Diff√©rence"))
    print("-" * 70)
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    for metric, name in zip(metrics, metric_names):
        sklearn_val = sklearn_metrics[metric]
        c_val = c_results[f'c_{metric}']
        diff = abs(sklearn_val - c_val)
        
        print("{:<20} {:<15.4f} {:<15.4f} {:<15.4f}".format(name, sklearn_val, c_val, diff))
    
    print("\n" + "=" * 60)
    
    # Analyse des diff√©rences
    print("\nüìä ANALYSE:")
    
    max_diff = max([abs(sklearn_metrics[m] - c_results[f'c_{m}']) for m in metrics])
    
    if max_diff < 0.01:
        print("‚úì Excellent ! Les r√©sultats sont quasiment identiques (diff√©rence < 1%)")
    elif max_diff < 0.05:
        print("‚úì Bon ! Les r√©sultats sont tr√®s similaires (diff√©rence < 5%)")
    elif max_diff < 0.10:
        print("‚ö† Les r√©sultats sont similaires mais avec quelques diff√©rences (< 10%)")
    else:
        print("‚ö† Les r√©sultats diff√®rent significativement (> 10%)")
    
    print("\nCauses possibles de diff√©rences:")
    print("  - Diff√©rences d'optimisation (L-BFGS vs Gradient Descent)")
    print("  - Pr√©cision num√©rique (float vs double)")
    print("  - Initialisation al√©atoire du split train/test")
    print("  - Nombre d'it√©rations diff√©rent avant convergence")

def save_comparison_report(sklearn_metrics, c_results):
    """Sauvegarde un rapport de comparaison"""
    output_path = Path("results/sklearn_comparison.txt")
    
    with open(output_path, 'w') as f:
        f.write("=" * 60 + "\n")
        f.write("COMPARAISON SCIKIT-LEARN VS IMPL√âMENTATION C\n")
        f.write("=" * 60 + "\n\n")
        
        if c_results:
            f.write("{:<20} {:<15} {:<15} {:<15}\n".format("M√©trique", "Scikit-learn", "C (custom)", "Diff√©rence"))
            f.write("-" * 70 + "\n")
            
            metrics = ['accuracy', 'precision', 'recall', 'f1']
            metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
            
            for metric, name in zip(metrics, metric_names):
                sklearn_val = sklearn_metrics[metric]
                c_val = c_results[f'c_{metric}']
                diff = abs(sklearn_val - c_val)
                
                f.write("{:<20} {:<15.4f} {:<15.4f} {:<15.4f}\n".format(name, sklearn_val, c_val, diff))
    
    print(f"\n‚úì Rapport sauvegard√©: {output_path}")

def main():
    """Fonction principale"""
    print("\n" + "=" * 60)
    print("VALIDATION AVEC SCIKIT-LEARN")
    print("=" * 60 + "\n")
    
    # Charger et pr√©traiter
    X, y = load_and_preprocess_data()
    
    # Split train/test (80/20 comme dans le code C)
    print("\n‚úÇÔ∏è Split train/test (80/20)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"‚úì Train: {len(X_train)} samples, Test: {len(X_test)} samples")
    
    # Entra√Æner le mod√®le sklearn
    model, scaler, X_train_scaled, X_test_scaled = train_sklearn_model(
        X_train, y_train, X_test, y_test
    )
    
    # Pr√©dictions
    print("\nüîÆ Pr√©dictions sur le test set...")
    y_pred = model.predict(X_test_scaled)
    
    # Calculer les m√©triques
    sklearn_metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1': f1_score(y_test, y_pred, zero_division=0)
    }
    
    print("\nüìä M√©triques scikit-learn:")
    print(f"  Accuracy:  {sklearn_metrics['accuracy']:.4f}")
    print(f"  Precision: {sklearn_metrics['precision']:.4f}")
    print(f"  Recall:    {sklearn_metrics['recall']:.4f}")
    print(f"  F1-Score:  {sklearn_metrics['f1']:.4f}")
    
    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n  Confusion Matrix:")
    print(f"    TN: {cm[0,0]}, FP: {cm[0,1]}")
    print(f"    FN: {cm[1,0]}, TP: {cm[1,1]}")
    
    # Charger et comparer avec les r√©sultats C
    c_results = load_c_results()
    compare_results(sklearn_metrics, c_results)
    save_comparison_report(sklearn_metrics, c_results)
    
    print("\n" + "=" * 60)
    print("‚úì COMPARAISON TERMIN√âE")
    print("=" * 60 + "\n")

if __name__ == "__main__":
    main()

